{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어적 평가 AI 모델(KLUE-BERT) - 유효성 검증 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1단계: 필요 라이브러리 설치 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\r\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\r\n",
      "Cuda compilation tools, release 11.8, V11.8.89\r\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "\n",
    "# 2.0.1+cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 15:35:38.176120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "# 2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cudnn-cu11==8.6.0.163 in /usr/local/lib/python3.8/dist-packages (8.6.0.163)\n",
      "Requirement already satisfied: nvidia-cublas-cu11 in /usr/local/lib/python3.8/dist-packages (from nvidia-cudnn-cu11==8.6.0.163) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11->nvidia-cudnn-cu11==8.6.0.163) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11->nvidia-cudnn-cu11==8.6.0.163) (0.40.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nvidia-cudnn-cu11==8.6.0.163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9CUA77n7S77P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tsazkyKgZaz0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.8/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XXN8du1pYzM0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import shape_list, BertTokenizer, TFBertModel\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2단계: 데이터 준비 - 라벨링 데이터 로드(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링 데이터 가져오는 함수\n",
    "def get_data(data, path):\n",
    "    file_name, presen_script, script_stt_txt, script_tag_txt = [], [], [], []  # 빈 리스트 생성 (6개)\n",
    "    for row in tqdm(data.itertuples(), total=data.shape[0]):  # 진행상황 확인을 위한 tqdm, data frame 값을 빠르게 가져오기 위한 itertuples\n",
    "        file_str = path + row.file_name                       # 파일명\n",
    "\n",
    "        with open(file_str) as f:      # json파일 열기\n",
    "            text = json.load(f)\n",
    "\n",
    "        file_name.append(text['info']['filename'])  # filename\n",
    "        presen_script.append(text['presentation']['presen_script'])    # 발화원문\n",
    "        script_stt_txt.append(text['script']['script_stt_txt'])   # 발화 내용(STT)\n",
    "        script_tag_txt.append(text['script']['script_tag_txt'])    # 발화 내용(태그매핑)\n",
    "\n",
    "    df= pd.DataFrame({'file_name': file_name, 'presen_script': presen_script, 'script_stt_txt': script_stt_txt, 'script_tag_txt': script_tag_txt})  # 리스트로 dataframe 만들기\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-태그인식_데이터전처리코드_v0.1.ipynb\r\n",
      "10-태그인식_데이터전처리코드_v0.2.ipynb\r\n",
      "10-태그인식_데이터전처리코드_v0.3.ipynb\r\n",
      "20-KLUE_BERT를_이용한_태그_인식_customizing_v0.1.ipynb\r\n",
      "20-KLUE_BERT를_이용한_태그_인식_customizing_v0.12.ipynb\r\n",
      "20-KLUE_BERT를_이용한_태그_인식_customizing_v0.13.ipynb\r\n",
      "20-KLUE_BERT를_이용한_태그_인식_customizing_v0.3.ipynb\r\n",
      "20-KLUE_BERT를_이용한_태그_인식_valid_customizing_v0.11.ipynb\r\n",
      "Dockerfile\r\n",
      "KLUE_BERT를_이용한_태그_인식_customizing_v0.14.ipynb\r\n",
      "NER\r\n",
      "dataset\r\n",
      "dataset_2309\r\n",
      "dataset_2310_556e\r\n",
      "dataset_2310_636e\r\n",
      "docker\r\n",
      "docker-compose.yml\r\n",
      "docker_lang.tar\r\n",
      "klue-bert-pytorch.ipynb\r\n",
      "klue-bert.py\r\n",
      "logs\r\n",
      "main_df_csv_800e.csv\r\n",
      "ner_label_v2.txt\r\n",
      "nia89_lang_workspace.tar\r\n",
      "nia89_workspace\r\n",
      "nia89_workspace_test\r\n",
      "old\r\n",
      "old_data\r\n",
      "requirements.txt\r\n",
      "results\r\n",
      "sample_docker_test\r\n",
      "saved_model\r\n",
      "summarybot\r\n",
      "tag_real_test_data_100.csv\r\n",
      "tag_real_test_data_11.csv\r\n",
      "tag_real_test_data_13_norandom.csv\r\n",
      "tag_real_test_data_99.csv\r\n",
      "tag_real_train_data_100.csv\r\n",
      "tag_real_train_data_11.csv\r\n",
      "tag_real_train_data_13_norandom.csv\r\n",
      "tag_real_train_data_99.csv\r\n",
      "tag_test_data.csv\r\n",
      "tag_test_data_2155e.csv\r\n",
      "tag_test_data_2163e.csv\r\n",
      "tag_test_data_347e\r\n",
      "tag_test_data_347e.csv\r\n",
      "tag_test_data_357e.csv\r\n",
      "tag_test_data_556e.csv\r\n",
      "tag_test_data_636e.csv\r\n",
      "tag_test_data_800e.csv\r\n",
      "tag_test_data_jebal.csv\r\n",
      "tag_test_data_test.csv\r\n",
      "tag_test_only_data33.csv\r\n",
      "tag_test_ordata_347e.csv\r\n",
      "tag_test_orgdata_347e.csv\r\n",
      "tag_test_orgdata_357e.csv\r\n",
      "tag_train_data.csv\r\n",
      "tag_train_data_17290e.csv\r\n",
      "tag_train_data_17307e.csv\r\n",
      "tag_train_data_347e\r\n",
      "tag_train_data_347e.csv\r\n",
      "tag_train_data_357e.csv\r\n",
      "tag_train_data_556e.csv\r\n",
      "tag_train_data_636e.csv\r\n",
      "tag_train_data_800e.csv\r\n",
      "tag_train_data_jebal.csv\r\n",
      "tag_train_data_test.csv\r\n",
      "tag_train_only_data.csv\r\n",
      "tag_train_only_data22.csv\r\n",
      "tag_train_only_data33.csv\r\n",
      "tag_train_ordata_347e.csv\r\n",
      "tag_train_orgdata_347e.csv\r\n",
      "tag_train_orgdata_357e.csv\r\n",
      "tag_valid_data.csv\r\n",
      "tag_valid_data_2155e.csv\r\n",
      "tag_valid_data_2164e.csv\r\n",
      "tag_valid_data_800e.csv\r\n",
      "testdockerfile\r\n",
      "training_1\r\n",
      "training_2\r\n",
      "venv\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 2387.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>presen_script</th>\n",
       "      <th>script_stt_txt</th>\n",
       "      <th>script_tag_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01_S02_M_A_08_019_02_WA_MO_presentation.json</td>\n",
       "      <td>과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 현상은 인류에게 큰 ...</td>\n",
       "      <td>과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는현상을인류에게 큰 변화...</td>\n",
       "      <td>과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 &lt;WR&gt;현상을&lt;/WR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A05_S02_F_F_01_092_02_WA_MO_presentation.json</td>\n",
       "      <td>지적 재산권은 현대 사회에서 매우 중요한 개념으로 여겨지며, 기술과 창의성의 보호 ...</td>\n",
       "      <td>지적 재산권은 현대 사회에서 매우 중요한 개념으로 여겨지며 기술과 창의성의 보호 및...</td>\n",
       "      <td>지적 재산권은 현대 사회에서 매우 중요한 개념으로 여겨지며 기술과 창의성의 보호 및...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A03_S03_M_F_05_100_02_WA_MO_presentation.json</td>\n",
       "      <td>비혼주의는 혼인을 피하고 혼자 또는 동반자와 함께 삶을 즐기는 사상이나 생활 방식을...</td>\n",
       "      <td>비혼주의는 혼인을 피하고 혼자 또는 동반자와 함께 삶은 즐기는 사상이나 생활 방식을...</td>\n",
       "      <td>비혼주의는 혼인을 피하고 혼자 또는 동반자와 함께  &lt;WR&gt;삶은&lt;/WR&gt;  즐기는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01_S01_F_C_01_045_02_WA_MO_presentation.json</td>\n",
       "      <td>안녕하세요, 여러분. 오늘은 저의 미래에 대해 이야기하고자 합니다. 제가 가지고 있...</td>\n",
       "      <td>안녕하세요, 여러분. 오늘은 저의 미래에 대해 이야기하고자 합니다.제가 가지고 있는...</td>\n",
       "      <td>안녕하세요, 여러분. 오늘은 저의 미래에 대해 이야기하고자 합니다.제가 가지고 있는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01_S02_M_A_04_011_02_WA_MO_presentation.json</td>\n",
       "      <td>안녕하세요, 자동차 및 운송수단의 발전이 우리의 삶에 미치는 부정적인 영향에 대해 ...</td>\n",
       "      <td>안녕하세요. 자동차 및 운송수단의 발전이 우리의 삶에 영향이 미치는 부정적인 영향에...</td>\n",
       "      <td>안녕하세요. 자동차 및 운송수단의 발전이 우리의 삶에 영향이 미치는 부정적인 영향에...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_name  \\\n",
       "0  A01_S02_M_A_08_019_02_WA_MO_presentation.json   \n",
       "1  A05_S02_F_F_01_092_02_WA_MO_presentation.json   \n",
       "2  A03_S03_M_F_05_100_02_WA_MO_presentation.json   \n",
       "3  A01_S01_F_C_01_045_02_WA_MO_presentation.json   \n",
       "4  A01_S02_M_A_04_011_02_WA_MO_presentation.json   \n",
       "\n",
       "                                       presen_script  \\\n",
       "0  과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 현상은 인류에게 큰 ...   \n",
       "1  지적 재산권은 현대 사회에서 매우 중요한 개념으로 여겨지며, 기술과 창의성의 보호 ...   \n",
       "2  비혼주의는 혼인을 피하고 혼자 또는 동반자와 함께 삶을 즐기는 사상이나 생활 방식을...   \n",
       "3  안녕하세요, 여러분. 오늘은 저의 미래에 대해 이야기하고자 합니다. 제가 가지고 있...   \n",
       "4  안녕하세요, 자동차 및 운송수단의 발전이 우리의 삶에 미치는 부정적인 영향에 대해 ...   \n",
       "\n",
       "                                      script_stt_txt  \\\n",
       "0  과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는현상을인류에게 큰 변화...   \n",
       "1  지적 재산권은 현대 사회에서 매우 중요한 개념으로 여겨지며 기술과 창의성의 보호 및...   \n",
       "2  비혼주의는 혼인을 피하고 혼자 또는 동반자와 함께 삶은 즐기는 사상이나 생활 방식을...   \n",
       "3  안녕하세요, 여러분. 오늘은 저의 미래에 대해 이야기하고자 합니다.제가 가지고 있는...   \n",
       "4  안녕하세요. 자동차 및 운송수단의 발전이 우리의 삶에 영향이 미치는 부정적인 영향에...   \n",
       "\n",
       "                                      script_tag_txt  \n",
       "0  과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 <WR>현상을</WR...  \n",
       "1  지적 재산권은 현대 사회에서 매우 중요한 개념으로 여겨지며 기술과 창의성의 보호 및...  \n",
       "2  비혼주의는 혼인을 피하고 혼자 또는 동반자와 함께  <WR>삶은</WR>  즐기는 ...  \n",
       "3  안녕하세요, 여러분. 오늘은 저의 미래에 대해 이야기하고자 합니다.제가 가지고 있는...  \n",
       "4  안녕하세요. 자동차 및 운송수단의 발전이 우리의 삶에 영향이 미치는 부정적인 영향에...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    " # [주의] dataset폴더 안에 라벨링데이터(json) 파일 저장\n",
    "paths = 'dataset/'\n",
    "fileNameList = listdir(paths)\n",
    "len(fileNameList)\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "df= pd.DataFrame(fileNameList, columns=['file_name'])     # 파일 리스트 -> 데이터 프레임화\n",
    "df=df.astype('string')                                    # df type string 으로 변경\n",
    "\n",
    "cr = df['file_name'].str.contains('A') # presentation\n",
    "data = df[cr]\n",
    "\n",
    "sub_df = get_data(data, paths)                          # 데이터 가져오는 함수\n",
    "main_df = pd.concat([main_df, sub_df])            # main_labels에 누적하여 더하기\n",
    "main_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 라벨링데이터셋을 CSV 파일로 저장\n",
    "main_df_csv = pd.DataFrame(main_df, columns=[\"file_name\", \"presen_script\", \"script_stt_txt\", \"script_tag_txt\"])\n",
    "main_df_csv.to_csv(\"main_df_csv_800e.csv\", encoding=\"utf-8-sig\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3단계: 데이터 가공 - 태그 시퀀스 레이블링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def label_text(text):\n",
    "    labels = []\n",
    "    words = text.split()\n",
    "    \n",
    "    for word in words:\n",
    "        # 개체명 인식\n",
    "        if  word.startswith(\"<\") or word.endswith(\"/>\"):\n",
    "            labels.append(word)\n",
    "        elif word.startswith(\"자\") and word.endswith(\">\"):  # (예외:태그구문규칙 불일치) 자신의</REP><WR>이미지</WR><WR>강조하고</WR><WR>있는</WR> -> 자신의</REP> <WR>이미지</WR> <WR>강조하고</WR> <WR>있는</WR>\n",
    "            labels.append(word)\n",
    "        elif word == '다양성과<FIL>': # (예외:태그구문규칙 불일치) 다양성과<FIL> 포용을 </FIL>훼손하며 -> 다양성과 <FIL>포용을</FIL> 훼손하며\n",
    "            labels.append(word)\n",
    "        # 개체명이 아닌 경우 O 태그 부여\n",
    "        else:\n",
    "            labels.append('O')\n",
    "\n",
    "    return labels\n",
    " \n",
    "def merge_tags(text):\n",
    "    text = re.sub(r'\\*', '', text.strip()) # *O\n",
    "    text = re.sub(r'\\*\\w', 'O', text.strip()) # *O\n",
    "    text = re.sub(r'<REP>\\w\\*\\w', 'REP-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR><PS/><WR>\\w+</WR>', 'WR-B PS-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<PS/><WR>\\w+</WR>', 'PS-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w</FIL><PS/><FIL>\\w</FIL>', 'FIL-B PS-B FIL-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR></REP><FIL>\\w</FIL>', 'WR-B FIL-B', text.strip()) \n",
    "    text = re.sub(r'<PS/><FIL>\\w</FIL>', 'PS-B FIL-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><PS/><WR>\\w+</WR>', 'WR-B PS-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w<FIL>\\w</FIL>\\w+</REP>', 'REP-B FIL-B O', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w</FIL>\\w+<FIL>\\w</FIL><REP>\\w', 'FIL-B O FIL-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<FIL>\\w+</FIL>', 'REP-B FIL-B', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w+</FIL><PS/><FIL>\\w+</FIL>', 'FIL-B PS-B FIL-B ', text.strip()) \n",
    "    text = re.sub(r'<REP><WR>\\w+</WR><FIL>\\w+</FIL>', 'REP-B WR-B FIL-B ', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR></REP>\\w+<WR></WR><FIL>\\w+</FIL><REP>\\w+', 'WR-B O WR-B FIL-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+(-)\\w+</WR>', 'WR-B', text.strip())\n",
    "    text = re.sub(r'<REP><WR>\\w+</WR><FIL>\\w+</FIL>\\w+</REP>', 'REP-B WR-B FIL-B O', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR></REP><WR>\\w+</WR>', 'WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR></REP><WR>\\w+</WR><REP>\\w+</REP>', 'REP-B WR-B WR-B REP-B', text.strip())\n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR></REP><WR>\\w+</WR>', 'REP-B WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR>\\w+', 'REP-B WR-B O', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w+</FIL><WR>\\w+</WR></REP>', 'FIL-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w+</FIL><WR>\\w+</WR><REP>\\w', 'FIL-B WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+</REP><REP>\\w+<WR>\\w+</WR></REP>', 'REP-B REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR>\\w+<WR>\\w+</WR><REP>\\w+', 'WR-B O WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR>\\w+<WR>\\w+</WR><WR>\\w+</WR>', 'WR-B O WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><FIL>\\w+</FIL>\\w+', 'WR-B FIL-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR>\\w+<WR>\\w+</WR>', 'WR-B WR-B O WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR></REP>\\w+<WR></WR>', 'WR-B O WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR></REP>', 'WR-B O', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR><WR>\\w+</WR><REP>\\w+<WR>\\w+</WR>', 'WR-B WR-B WR-B REP-B WR-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR><REP>\\w+', 'WR-B WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP><WR>\\w+</WR>', 'REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR>', 'REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'\\w+</WR></REP> ', 'O', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+', 'REP-B WR-B', text.strip())\n",
    "    text = re.sub(r'<PS/><WR></WR><FIL>\\w+</FIL>', 'PS-B WR-B FIL-B', text.strip())\n",
    "    text = re.sub(r'<WR><REP>\\w+', 'WR-B REP-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR><PS/>', 'WR-B PS-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR><REP>\\w+</REP><WR>\\w+</WR>', 'WR-B REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w+</FIL>\\w+</REP>\\w+<FIL>\\w+</FIL><WR>\\w+</WR>', 'FIL-B O FIL-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><REP>\\w', 'WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR></REP><WR>\\w+</WR><REP>\\w+</REP>', 'REP-B WR-B WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR><REP>\\w+', 'WR-B WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR><WR>\\w+</WR><REP>\\w+<WR>\\w+</WR>', 'WR-B WR-B WR-B REP-B WR-B', text.strip())\n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR>\\w+', 'REP-B WR-B O', text.strip()) \n",
    "    text = re.sub(r'<WR>(.*?)</WR><REP>(.*?)</REP><WR>(.*?)</WR>', 'WR-B REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+</REP><WR>\\w+<WR>\\w+</WR></WR><REP>\\w+', 'REP-B WR-B WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w+</FIL><WR>\\w+</WR><WR>\\w+</WR><WR>\\w+</WR>', 'FIL-B WR-B WR-B WR-B', text.strip())\n",
    "    text = re.sub(r'<FIL>\\w+<FIL/><PS/><FIL>\\w+</FIL>', 'FIL-B PS-B FIL-B', text.strip())\n",
    "    text = re.sub(r'<FIL>\\w+</FIL><PS/>', 'FIL-B PS-B', text.strip())\n",
    "    text = re.sub(r'<<FIL>\\w+</FIL>', 'FIL-B', text.strip())\n",
    "    text = re.sub(r'\\w+<PS/>', 'PS-B', text.strip())\n",
    "    text = re.sub(r'<PS/>', 'PS-B', text.strip())\n",
    "    text = re.sub(r'<PS/>\\w+', 'PS-B O', text.strip())\n",
    "    text = re.sub(r'PS-B\\w+', 'PS-B O', text.strip())\n",
    "    text = re.sub(r'\\w+<FIL>', 'FIL-B', text.strip()) \n",
    "    text = re.sub(r'<FIL>\\w+</FIL>\\w+', 'FIL-B O', text.strip())\n",
    "    text = re.sub(r'<FIL>\\s*(.*?)\\s*</FIL>', r' <FIL>\\1</FIL> ', text.strip())\n",
    "    text = re.sub(r'<FIL>(.*?)</FIL>', 'FIL-B', text.strip())\n",
    "    text = re.sub(r'<FIL>(.*?) ', 'FIL-B ', text.strip())\n",
    "    text = re.sub(r'<FIL>', 'FIL-B', text.strip())\n",
    "    text = re.sub(r'</FIL>\\w+', 'O', text.strip()) \n",
    "    text = re.sub(r\"<FIL>\\w+|FIL\\w+|FIL \\w+|FIL,\", \"FIL-B\", text.strip()).replace(\",\", \"\")\n",
    "    text = re.sub(r'<REP>\\w+(\")\\w+</REP>', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+</REP>', 'REP-B', text.strip())\n",
    "    text = re.sub(r'<REP><WR>\\w+', 'REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+-\\w+', 'REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'</REP><REP>\\w+', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'</REP>자', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'</REP>창', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+<WR>\\w+</WR></REP><WR>\\w+</WR>', 'REP-B WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>(.*?)</REP>', 'REP-B', text.strip())\n",
    "    text = re.sub(r'<REP>\\w+ <PS/>', 'REP-B PS-B', text.strip()) \n",
    "    text = re.sub(r'\\w+</REP><WR>\\w+</WR><WR>\\w+</WR><WR>\\w+</WR>', 'O WR-B WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>\\w+</REP><REP>\\w+<WR>\\w+</WR></REP>', 'REP-B REP-B WR-B', text.strip()) \n",
    "    text = re.sub(r'<REP>[가-힣?]+', 'REP-B', text.strip())\n",
    "    text = re.sub(r'<REP> <REP>', 'REP-B', text.strip())\n",
    "    text = re.sub(r'<REP>[A-Za-z0-9_-가-힣]+', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'REP-B\\w+', 'REP-B ', text.strip())\n",
    "    text = re.sub(r'<REP>\\s*(.*?)\\s*</REP>', r' <REP>\\1</REP> ', text.strip())\n",
    "    text = re.sub(r'<REP>(.*?)</REP>', 'REP-B', text.strip())\n",
    "    text = re.sub(r\"<REP>\\w+|REP\\w+|REP \\w+|REP,\", \"REP-B\", text).replace(\",\", \"\")\n",
    "    text = re.sub(r'\\w+<REP>', 'REP-B', text.strip())\n",
    "    text = re.sub(r'</REP>\\w+','O', text.strip()) \n",
    "    text = re.sub(r'\\w+</REP>','O', text.strip()) \n",
    "    text = re.sub(r'</REP>','', text.strip())\n",
    "    text = re.sub(r'<REP>', 'REP-B', text.strip()) \n",
    "    text = re.sub(r'<R','', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR>', 'WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'</WR></WR>\\w+', 'O', text.strip()) \n",
    "    text = re.sub(r'</WR>\\w+', 'O', text.strip())\n",
    "    text = re.sub(r'\\w+</WR>', 'O', text.strip())\n",
    "    text = re.sub(r'</WR>', 'O', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR><WR>\\w+</WR><WR>\\w+</WR>', 'WR-B WR-B WR-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR>\\w+', 'WR-B O', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+</WR>(.*?)', 'WR-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\w+', 'WR-B', text.strip())\n",
    "    text = re.sub(r'<WR>\\s*(.*?)\\s*</WR>', r' <WR>\\1</WR> ', text.strip())\n",
    "    text = re.sub(r'<WR>(.*?)</WR>', 'WR-B', text.strip())\n",
    "    text = re.sub(r'<WR>(.*?)</WR></REP>', 'WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>(.*?)</WR>(.*?)', 'WR-B', text.strip()) \n",
    "    text = re.sub(r'<WR>(.*?) ', 'WR-B ', text.strip())\n",
    "    text = re.sub(r'</WR>(.*?)</WR>', 'WR-B', text.strip()) \n",
    "    text = re.sub(r'</WR>','', text.strip())\n",
    "    text = re.sub(r'</WR></WR>\\w+', 'WR-B', text.strip()) \n",
    "    text = re.sub(r'</WR></WR>', '', text.strip()) \n",
    "    text = re.sub(r'</WR>\\w+</WR>', 'WR-B', text.strip()) \n",
    "    text = re.sub(r\"<WR>\\w+|WR\\w+|WR \\w+|WR,\", \"WR-B\", text.strip()).replace(\",\", \"\")\n",
    "    text = re.sub(r'</WR>\\w+', 'O', text.strip())\n",
    "    text = re.sub(r'<WR>', 'WR-B', text.strip())\n",
    "    text = re.sub(r'WR-B</WR', 'WR-B', text.strip())\n",
    "    text = re.sub(r'WR-BWR-B', 'WR-B WR-B', text.strip()) \n",
    "    text = re.sub(r'WR-B O-B', 'WR-B REP-B', text.strip()) \n",
    "    text = re.sub(r'<', 'O', text.strip())\n",
    "    text = re.sub(r'REP-B?', 'REP-B ', text.strip())\n",
    "    text = re.sub(r'WR-B?', 'WR-B ', text.strip())\n",
    "    text = re.sub(r\"'\\w\", 'O', text.strip())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4단계: 문장 태그 정제 및 공백행 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_tags(df):\n",
    "    # 문장 태그 정제\n",
    "    search_patterns = ['<REP>', '</REP>', '<WR>', '</WR>', '<FIL>', '</FIL>', '?']\n",
    "    searchtag_patterns = ['?', \"'\", ',']\n",
    "    \n",
    "    for pattern in search_patterns:\n",
    "        df['Sentence'] = df['Sentence'].str.replace(pattern, '')\n",
    "        \n",
    "    for pattern in searchtag_patterns:\n",
    "        df['Tag'] = df['Tag'].str.replace(pattern, '')\n",
    "\n",
    "    # 공백이 있는 행 제거\n",
    "    df = df[df['Sentence'].str.strip() != '']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5단계: 데이터 가공 - 학습/테스트 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "# 데이터셋 비율 설정\n",
    "train_ratio = 0.8  # 학습 데이터셋 비율\n",
    "valid_ratio = 0.1  # 검증 데이터셋 비율\n",
    "\n",
    "sentences = []\n",
    "\n",
    "# 문장을 리스트로 추출\n",
    "for text in main_df['script_tag_txt']:\n",
    "    sentences.extend(text.split('.'))\n",
    "\n",
    "# [주의] 랜덤으로 문장 순서 섞도록 설정 가능\n",
    "#shuffle(sentences)\n",
    "\n",
    "# 분할 인덱스 계산\n",
    "train_idx = int(len(sentences) * train_ratio)\n",
    "valid_idx = int(len(sentences) * (train_ratio + valid_ratio))\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "train_data = []\n",
    "for sentence in sentences[:train_idx]:\n",
    "    if sentence.strip():\n",
    "        train_data.append([sentence.strip(), merge_tags(' '.join(label_text(sentence)))])\n",
    "\n",
    "# 검증 데이터셋 생성\n",
    "valid_data = []\n",
    "for sentence in sentences[train_idx:valid_idx]:\n",
    "    if sentence.strip():\n",
    "        valid_data.append([sentence.strip(), merge_tags(' '.join(label_text(sentence)))])\n",
    "        \n",
    "# 테스트 데이터셋 생성\n",
    "test_data = []\n",
    "for sentence in sentences[valid_idx:]:\n",
    "    if sentence.strip():\n",
    "        test_data.append([sentence.strip(), merge_tags(' '.join(label_text(sentence)))])\n",
    "\n",
    "# 학습 데이터셋을 CSV 파일로 저장\n",
    "df_train = pd.DataFrame(train_data, columns=[\"Sentence\", \"Tag\"])\n",
    "df_train = cleanse_tags(df_train)\n",
    "# [주의] 데이터전처리 완료된 학습데이터 파일(csv)을 dataset_tagsequence 폴더에 저장되도록 경로설정\n",
    "df_train.to_csv(\"tag_train_data_17290e.csv\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "# 검증 데이터셋을 CSV 파일로 저장\n",
    "df_valid = pd.DataFrame(valid_data, columns=[\"Sentence\", \"Tag\"])\n",
    "df_valid = cleanse_tags(df_valid)\n",
    "# [주의] 데이터전처리 완료된 검증데이터 파일(csv)을 dataset_tagsequence 폴더에 저장되도록 경로설정\n",
    "df_valid.to_csv(\"tag_valid_data_2164e.csv\", encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "# 테스트 데이터셋을 CSV 파일로 저장\n",
    "df_test = pd.DataFrame(test_data, columns=[\"Sentence\", \"Tag\"])\n",
    "df_test = cleanse_tags(df_test)\n",
    "# [주의] 데이터전처리 완료된 테스트데이터 파일(csv)을 dataset_tagsequence 폴더에 저장되도록 경로설정\n",
    "df_test.to_csv(\"tag_test_data_2163e.csv\", encoding=\"utf-8-sig\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 현상을 인류에게 큰 ...</td>\n",
       "      <td>O O O O O O O O WR-B  O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오랜 세월 동안 인간의 수명은 다양한 요인에 영향을 받아 변화해왔지만, 현재는  의...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이러한 평균 수명의 연장이 우리의 삶에 어떤 영향을 주는지 살펴보고자 합니다</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>첫째로, 평균 수명의 연장은 개인의 삶의 질을 향상시킵니다</td>\n",
       "      <td>O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>더 오래 살 수 있는 기회는 건강한 삶을 누리는데 큰 도움이 됩니다</td>\n",
       "      <td>O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 현상을 인류에게 큰 ...   \n",
       "1  오랜 세월 동안 인간의 수명은 다양한 요인에 영향을 받아 변화해왔지만, 현재는  의...   \n",
       "2         이러한 평균 수명의 연장이 우리의 삶에 어떤 영향을 주는지 살펴보고자 합니다   \n",
       "3                   첫째로, 평균 수명의 연장은 개인의 삶의 질을 향상시킵니다   \n",
       "4              더 오래 살 수 있는 기회는 건강한 삶을 누리는데 큰 도움이 됩니다   \n",
       "\n",
       "                                               Tag  \n",
       "0                    O O O O O O O O WR-B  O O O O  \n",
       "1  O O O O O O O O O O O O O O O O O O O O O O O O  \n",
       "2                            O O O O O O O O O O O  \n",
       "3                                  O O O O O O O O  \n",
       "4                          O O O O O O O O O O O O  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어  금융 관리는 우리 삶에서  어   음  중요한 부분을 차지하며 미래를 준비하는...</td>\n",
       "      <td>FIL-B  O O O O  FIL-B   FIL-B  O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어  계획적인  대출, 대출과  카드 사용, 현명한 소비 습관, 금융 계획 수립 등...</td>\n",
       "      <td>FIL-B  O REP-B  O O O O O O O O O O O O O O  F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어  대출과 신용카드 사용은  어   현명 현명하게  계획하고 관리해야 하는 중요한...</td>\n",
       "      <td>FIL-B  O O O  FIL-B  REP-B  O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>금융   저  건강 건강을  유지하려면 신중한 판단과 계획, 책임적인 사용이  어 ...</td>\n",
       "      <td>O REP-B   FIL-B  O O O O O O O O  FIL-B  O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>우리는 자신의 재정 상황을 잘 이해하고 대출과 카드 사용을  어   신중 신중하게 ...</td>\n",
       "      <td>O O O O O O O O O  FIL-B  REP-B  O REP-B  WR-B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  어  금융 관리는 우리 삶에서  어   음  중요한 부분을 차지하며 미래를 준비하는...   \n",
       "1  어  계획적인  대출, 대출과  카드 사용, 현명한 소비 습관, 금융 계획 수립 등...   \n",
       "2  어  대출과 신용카드 사용은  어   현명 현명하게  계획하고 관리해야 하는 중요한...   \n",
       "3  금융   저  건강 건강을  유지하려면 신중한 판단과 계획, 책임적인 사용이  어 ...   \n",
       "4  우리는 자신의 재정 상황을 잘 이해하고 대출과 카드 사용을  어   신중 신중하게 ...   \n",
       "\n",
       "                                                 Tag  \n",
       "0     FIL-B  O O O O  FIL-B   FIL-B  O O O O O O O O  \n",
       "1  FIL-B  O REP-B  O O O O O O O O O O O O O O  F...  \n",
       "2            FIL-B  O O O  FIL-B  REP-B  O O O O O O  \n",
       "3         O REP-B   FIL-B  O O O O O O O O  FIL-B  O  \n",
       "4  O O O O O O O O O  FIL-B  REP-B  O REP-B  WR-B...  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>부부로서의 가족 책임과 의무를 피하고 개인적인 목표와 꿈에 집중할 수 있기 때문입니다</td>\n",
       "      <td>O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경제적인 압박을 덜 받으면서 자신만의 경제적 목표를 추구하며새로운 아이디어와 상업을...</td>\n",
       "      <td>O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이는 산업발전과 창조적인 활동을 부추길 수 있는 긍정적인 영향을 미칩니다</td>\n",
       "      <td>O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>비혼은 더 높은 교육 수준과 전문성을 추구할 수 있는 기회를 제공합니다</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>가족의 책임이 줄어들어 시간과 에너지를 자기개발에 투자할 수 있기 때문입니다</td>\n",
       "      <td>O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0    부부로서의 가족 책임과 의무를 피하고 개인적인 목표와 꿈에 집중할 수 있기 때문입니다   \n",
       "1  경제적인 압박을 덜 받으면서 자신만의 경제적 목표를 추구하며새로운 아이디어와 상업을...   \n",
       "2           이는 산업발전과 창조적인 활동을 부추길 수 있는 긍정적인 영향을 미칩니다   \n",
       "3            비혼은 더 높은 교육 수준과 전문성을 추구할 수 있는 기회를 제공합니다   \n",
       "4         가족의 책임이 줄어들어 시간과 에너지를 자기개발에 투자할 수 있기 때문입니다   \n",
       "\n",
       "                             Tag  \n",
       "0        O O O O O O O O O O O O  \n",
       "1  O O O O O O O O O O O O O O O  \n",
       "2            O O O O O O O O O O  \n",
       "3          O O O O O O O O O O O  \n",
       "4            O O O O O O O O O O  "
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6단계: 학습데이터,검증데이터,테스트데이터 로드(load) 및 데이터전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "wHOJ0VIatlSl"
   },
   "outputs": [],
   "source": [
    "# [주의] 데이터전처리 완료된 학습데이터,검증데이터,테스트데이터 파일(csv)을 dataset_tagsequence 폴더에서 로드하기 위한 경로 설정 필요\n",
    "train_ner_df = pd.read_csv(\"tag_train_data_17290e.csv\")\n",
    "valid_ner_df = pd.read_csv(\"tag_valid_data_2164e.csv\")\n",
    "test_ner_df = pd.read_csv(\"tag_test_data_2163e.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "id": "9Ok5MPSSuxpt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 현상을 인류에게 큰 ...</td>\n",
       "      <td>O O O O O O O O WR-B  O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오랜 세월 동안 인간의 수명은 다양한 요인에 영향을 받아 변화해왔지만, 현재는  의...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이러한 평균 수명의 연장이 우리의 삶에 어떤 영향을 주는지 살펴보고자 합니다</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>첫째로, 평균 수명의 연장은 개인의 삶의 질을 향상시킵니다</td>\n",
       "      <td>O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>더 오래 살 수 있는 기회는 건강한 삶을 누리는데 큰 도움이 됩니다</td>\n",
       "      <td>O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  과학의 발전으로 현대인의 평균 수명이 지속적으로 증가하고 있는 현상을 인류에게 큰 ...   \n",
       "1  오랜 세월 동안 인간의 수명은 다양한 요인에 영향을 받아 변화해왔지만, 현재는  의...   \n",
       "2         이러한 평균 수명의 연장이 우리의 삶에 어떤 영향을 주는지 살펴보고자 합니다   \n",
       "3                   첫째로, 평균 수명의 연장은 개인의 삶의 질을 향상시킵니다   \n",
       "4              더 오래 살 수 있는 기회는 건강한 삶을 누리는데 큰 도움이 됩니다   \n",
       "\n",
       "                                               Tag  \n",
       "0                    O O O O O O O O WR-B  O O O O  \n",
       "1  O O O O O O O O O O O O O O O O O O O O O O O O  \n",
       "2                            O O O O O O O O O O O  \n",
       "3                                  O O O O O O O O  \n",
       "4                          O O O O O O O O O O O O  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어  금융 관리는 우리 삶에서  어   음  중요한 부분을 차지하며 미래를 준비하는...</td>\n",
       "      <td>FIL-B  O O O O  FIL-B   FIL-B  O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어  계획적인  대출, 대출과  카드 사용, 현명한 소비 습관, 금융 계획 수립 등...</td>\n",
       "      <td>FIL-B  O REP-B  O O O O O O O O O O O O O O  F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어  대출과 신용카드 사용은  어   현명 현명하게  계획하고 관리해야 하는 중요한...</td>\n",
       "      <td>FIL-B  O O O  FIL-B  REP-B  O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>금융   저  건강 건강을  유지하려면 신중한 판단과 계획, 책임적인 사용이  어 ...</td>\n",
       "      <td>O REP-B   FIL-B  O O O O O O O O  FIL-B  O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>우리는 자신의 재정 상황을 잘 이해하고 대출과 카드 사용을  어   신중 신중하게 ...</td>\n",
       "      <td>O O O O O O O O O  FIL-B  REP-B  O REP-B  WR-B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  어  금융 관리는 우리 삶에서  어   음  중요한 부분을 차지하며 미래를 준비하는...   \n",
       "1  어  계획적인  대출, 대출과  카드 사용, 현명한 소비 습관, 금융 계획 수립 등...   \n",
       "2  어  대출과 신용카드 사용은  어   현명 현명하게  계획하고 관리해야 하는 중요한...   \n",
       "3  금융   저  건강 건강을  유지하려면 신중한 판단과 계획, 책임적인 사용이  어 ...   \n",
       "4  우리는 자신의 재정 상황을 잘 이해하고 대출과 카드 사용을  어   신중 신중하게 ...   \n",
       "\n",
       "                                                 Tag  \n",
       "0     FIL-B  O O O O  FIL-B   FIL-B  O O O O O O O O  \n",
       "1  FIL-B  O REP-B  O O O O O O O O O O O O O O  F...  \n",
       "2            FIL-B  O O O  FIL-B  REP-B  O O O O O O  \n",
       "3         O REP-B   FIL-B  O O O O O O O O  FIL-B  O  \n",
       "4  O O O O O O O O O  FIL-B  REP-B  O REP-B  WR-B...  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "id": "WHiSNOEYu9bF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>부부로서의 가족 책임과 의무를 피하고 개인적인 목표와 꿈에 집중할 수 있기 때문입니다</td>\n",
       "      <td>O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경제적인 압박을 덜 받으면서 자신만의 경제적 목표를 추구하며새로운 아이디어와 상업을...</td>\n",
       "      <td>O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이는 산업발전과 창조적인 활동을 부추길 수 있는 긍정적인 영향을 미칩니다</td>\n",
       "      <td>O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>비혼은 더 높은 교육 수준과 전문성을 추구할 수 있는 기회를 제공합니다</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>가족의 책임이 줄어들어 시간과 에너지를 자기개발에 투자할 수 있기 때문입니다</td>\n",
       "      <td>O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0    부부로서의 가족 책임과 의무를 피하고 개인적인 목표와 꿈에 집중할 수 있기 때문입니다   \n",
       "1  경제적인 압박을 덜 받으면서 자신만의 경제적 목표를 추구하며새로운 아이디어와 상업을...   \n",
       "2           이는 산업발전과 창조적인 활동을 부추길 수 있는 긍정적인 영향을 미칩니다   \n",
       "3            비혼은 더 높은 교육 수준과 전문성을 추구할 수 있는 기회를 제공합니다   \n",
       "4         가족의 책임이 줄어들어 시간과 에너지를 자기개발에 투자할 수 있기 때문입니다   \n",
       "\n",
       "                             Tag  \n",
       "0        O O O O O O O O O O O O  \n",
       "1  O O O O O O O O O O O O O O O  \n",
       "2            O O O O O O O O O O  \n",
       "3          O O O O O O O O O O O  \n",
       "4            O O O O O O O O O O  "
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "id": "a1kAtJPCvINl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 샘플개수 : 17290\n",
      "검증데이터 샘플개수 : 2164\n",
      "테스트데이터 샘플개수 : 2163\n"
     ]
    }
   ],
   "source": [
    "print(\"학습데이터 샘플개수 :\", len(train_ner_df))\n",
    "print(\"검증데이터 샘플개수 :\", len(valid_ner_df))\n",
    "print(\"테스트데이터 샘플개수 :\", len(test_ner_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "id": "boweZqoDvOAE"
   },
   "outputs": [],
   "source": [
    "train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n",
    "valid_data_sentence = [sent.split() for sent in valid_ner_df['Sentence'].values]\n",
    "test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n",
    "train_data_label = [tag.split() for tag in train_ner_df['Tag'].values]\n",
    "valid_data_label = [tag.split() for tag in valid_ner_df['Tag'].values]\n",
    "test_data_label = [tag.split() for tag in test_ner_df['Tag'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "id": "GrcSGSobwEht"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이러한', '평균', '수명의', '연장이', '우리의', '삶에', '어떤', '영향을', '주는지', '살펴보고자', '합니다']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_sentence[2])\n",
    "print(train_data_label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "id": "-BqhwKF-w-KF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 태깅 정보 : ['O', 'FIL-B', 'REP-B', 'PS-B', 'WR-B']\n"
     ]
    }
   ],
   "source": [
    "# [주의] 태그가 정의된 파일(ner_label_v2.txt)을 dataset_nerlabel 폴더에서 로드하기 위한 경로 설정 필요\n",
    "labels = [label.strip() for label in open('ner_label_v2.txt', 'r', encoding='utf-8')]\n",
    "print('개체명 태깅 정보 :', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "id": "-hK9TKECxKDE"
   },
   "outputs": [],
   "source": [
    "tag_to_index = {tag: index for index, tag in enumerate(labels)}\n",
    "index_to_tag = {index: tag for index, tag in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "id": "YMO3BXIN2JEL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'FIL-B': 1, 'REP-B': 2, 'PS-B': 3, 'WR-B': 4}\n",
      "{0: 'O', 1: 'FIL-B', 2: 'REP-B', 3: 'PS-B', 4: 'WR-B'}\n"
     ]
    }
   ],
   "source": [
    "print(tag_to_index)\n",
    "print(index_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "id": "RuJ09NKk2LkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 태깅 정보의 개수 : 5\n"
     ]
    }
   ],
   "source": [
    "tag_size = len(tag_to_index)\n",
    "print('개체명 태깅 정보의 개수 :',tag_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토크나이저를 통한 형태소 분리 - KLUE-BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "id": "DN4Z3-zG2eW7"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "id": "39FhAHAF2fbi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 : ['오랜', '세월', '동안', '인간의', '수명은', '다양한', '요인에', '영향을', '받아', '변화해왔지만,', '현재는', '의료', '기술이', '의료', '기술의', '발전과', '생활환경의', '개선으로', '인해', '평균', '수명이', '급격히', '늘어나고', '있습니다']\n",
      "레이블 : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "레이블의 정수인코딩 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "문장의 길이 : 24\n",
      "레이블의 길이 : 24\n"
     ]
    }
   ],
   "source": [
    "sent = train_data_sentence[1]\n",
    "label = train_data_label[1]\n",
    "print('문장 :', sent)\n",
    "print('레이블 :',label)\n",
    "print('레이블의 정수인코딩 :',[tag_to_index[idx] for idx in label])\n",
    "print('문장의 길이 :', len(sent))\n",
    "print('레이블의 길이 :', len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "id": "gClkiKXI2p7D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 토 크 나 이 저 적 용 후 문 장 : ['오랜', '세월', '동안', '인간', '##의', '수명', '##은', '다양', '##한', '요인', '##에', '영향', '##을', '받아', '변화', '##해', '##왔', '##지만', ',', '현재', '##는', '의료', '기술', '##이', '의료', '기술', '##의', '발전', '##과', '생활', '##환경', '##의', '개선', '##으로', '인해', '평균', '수명', '##이', '급격히', '늘어나', '##고', '있', '##습', '##니다']\n",
      "레이블 : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "레이블의 정수 인코딩 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "문장의 길이 : 44\n",
      "레이블의 길이 : 24\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for one_word in sent:\n",
    "# 각 단어에 대해서 서브워드로 분리.\n",
    "# ex) one_word = '쿠 마 리' ===> subword_tokens = ['쿠', '##마 리']\n",
    "# ex) one_word = '한 동 수 가' ===> subword_tokens = ['한 동', '##수', '##가']\n",
    "  subword_tokens = tokenizer.tokenize(one_word)\n",
    "  tokens.extend(subword_tokens)\n",
    "print('BERT 토 크 나 이 저 적 용 후 문 장 :',tokens)\n",
    "print('레이블 :', label)\n",
    "print('레이블의 정수 인코딩 :',[tag_to_index[idx] for idx in label])\n",
    "print('문장의 길이 :', len(tokens))\n",
    "print('레이블의 길이 :', len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "id": "4EKLWdcC94-b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 후 문장 : ['오랜', '세월', '동안', '인간', '##의', '수명', '##은', '다양', '##한', '요인', '##에', '영향', '##을', '받아', '변화', '##해', '##왔', '##지만', ',', '현재', '##는', '의료', '기술', '##이', '의료', '기술', '##의', '발전', '##과', '생활', '##환경', '##의', '개선', '##으로', '인해', '평균', '수명', '##이', '급격히', '늘어나', '##고', '있', '##습', '##니다']\n",
      "레이블 : ['O', 'O', 'O', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', 'O', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', '[PAD]', 'O', '[PAD]', 'O', 'O', 'O', '[PAD]', 'O', 'O', '[PAD]', 'O', '[PAD]', '[PAD]']\n",
      "레이블의 정수 인코딩 : [0, 0, 0, 0, -100, 0, -100, 0, -100, 0, -100, 0, -100, 0, 0, -100, -100, -100, -100, 0, -100, 0, 0, -100, 0, 0, -100, 0, -100, 0, -100, -100, 0, -100, 0, 0, 0, -100, 0, 0, -100, 0, -100, -100]\n",
      "문장의 길이 : 44\n",
      "레이블의 길이 : 44\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "labels_ids = []\n",
    "for one_word, label_token in zip(train_data_sentence[1], train_data_label[1]):\n",
    "  subword_tokens = tokenizer.tokenize(one_word)\n",
    "  tokens.extend(subword_tokens)\n",
    "  labels_ids.extend([tag_to_index[label_token]]+ [-100] * (len(subword_tokens) -\n",
    "  1))\n",
    "print('토큰화 후 문장 :',tokens)\n",
    "print('레이블 :', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx in\n",
    "labels_ids])\n",
    "print('레이블의 정수 인코딩 :', labels_ids)\n",
    "print('문장의 길이 :', len(tokens))\n",
    "print('레이블의 길이 :', len(labels_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "id": "LaJ9naB4-3bh"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer,\n",
    "                                                  pad_token_id_for_segment=0,\n",
    "                                                  pad_token_id_for_label=-100):\n",
    "  cls_token = tokenizer.cls_token\n",
    "  sep_token = tokenizer.sep_token\n",
    "  pad_token_id = tokenizer.pad_token_id\n",
    "  input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "  for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "    tokens = []\n",
    "    labels_ids = []\n",
    "    for one_word, label_token in zip(example, label):\n",
    "      # 하나의 단어에 대해서 서브워드로 토큰화\n",
    "      subword_tokens = tokenizer.tokenize(one_word)\n",
    "      tokens.extend(subword_tokens)\n",
    "      # 서브워드 중 첫번째 서브워드만 개체명 레이블을 부여하고 그 외에는 -100으로 채운다.\n",
    "      labels_ids.extend([tag_to_index[label_token]]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
    "\n",
    "    # [CLS]와 [SEP]를 후에 추가할 것을 고려하여 최대길이를 초과하는 샘플의 경우 max_seq_len - 2의 길이로 변환.\n",
    "    # ex) max_seq_len = 64라면 길이가 62보다 긴 샘플은 뒷부분을 자르고 길이 62로 변환.\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > max_seq_len - special_tokens_count:\n",
    "      tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "      labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
    "    # [SEP]를 추가하는 코드\n",
    "    # 1. 토큰화 결과의 맨 뒷부분에 [SEP]토큰 추가\n",
    "    # 2. 레이블에도 맨 뒷부분에 -100 추가.\n",
    "    tokens += [sep_token]\n",
    "    labels_ids += [pad_token_id_for_label]\n",
    "\n",
    "    # [CLS]를 추가하는 코드\n",
    "    # 1. 토큰화 결과의 앞부분에 [CLS] 토큰 추가\n",
    "    # 2. 레이블의 맨 앞부분에도 -100 추가.\n",
    "    tokens = [cls_token] + tokens\n",
    "    labels_ids = [pad_token_id_for_label] + labels_ids\n",
    "\n",
    "    # 정수인코딩\n",
    "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # 어텐션마스크 생성\n",
    "    attention_mask = [1] * len(input_id)\n",
    "    # 정수인코딩에 추가할 패딩길이 연산\n",
    "    padding_count = max_seq_len - len(input_id)\n",
    "    # 정수인코딩 ,어텐션 마스크에 패딩 추가\n",
    "    input_id = input_id + ([pad_token_id] * padding_count)\n",
    "    attention_mask = attention_mask + ([0] * padding_count)\n",
    "    # 세그먼트 인코딩.\n",
    "    token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "    # 레이블 패딩. (단, 이 경우는 패딩 토큰의 ID가 -100)\n",
    "    label = labels_ids + ([pad_token_id_for_label] * padding_count)\n",
    "\n",
    "    assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "    assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "    assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "    assert len(label) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label), max_seq_len)\n",
    "\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "    data_labels.append(label)\n",
    "\n",
    "  input_ids = np.array(input_ids, dtype=int)\n",
    "  attention_masks = np.array(attention_masks, dtype=int)\n",
    "  token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "  data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "  return (input_ids, attention_masks, token_type_ids), data_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "id": "i9f1hOUBAPBS",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17290/17290 [00:06<00:00, 2578.61it/s]\n",
      "100%|██████████| 2164/2164 [00:00<00:00, 2541.44it/s]\n",
      "100%|██████████| 2163/2163 [00:00<00:00, 2568.41it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = convert_examples_to_features(train_data_sentence,\n",
    "train_data_label, max_seq_len=128, tokenizer=tokenizer)\n",
    "X_valid, y_valid = convert_examples_to_features(valid_data_sentence, valid_data_label,\n",
    "max_seq_len=128, tokenizer=tokenizer)\n",
    "X_test, y_test = convert_examples_to_features(test_data_sentence, test_data_label,\n",
    "max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "id": "q7Ge7QRgEoMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 원문 : ['과학의', '발전으로', '현대인의', '평균', '수명이', '지속적으로', '증가하고', '있는', '현상을', '인류에게', '큰', '변화를', '가져왔습니다']\n",
      "기존 레이블 : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'WR-B', 'O', 'O', 'O', 'O']\n",
      "--------------------------------------------------\n",
      "토큰화 후 원문 : ['[CLS]', '과학', '##의', '발전', '##으로', '현대인', '##의', '평균', '수명', '##이', '지속', '##적으로', '증가', '##하고', '있', '##는', '현상', '##을', '인류', '##에', '##게', '큰', '변화', '##를', '가져왔', '##습', '##니다', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "토큰화 후 레이블 : ['[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'O', '[PAD]', 'WR-B', '[PAD]', 'O', '[PAD]', '[PAD]', 'O', 'O', '[PAD]', 'O', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "--------------------------------------------------\n",
      "정수 인코딩 결과 : [    2  4038  2079  3859  6233 11443  2079  4233  9038  2052  4115 11187\n",
      "  3964 19521  1513  2259  4375  2069  5865  2170  2318  1751  3908  2138\n",
      " 11302  2219  3606     3     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "정수 인코딩 레이블 : [-100    0 -100    0 -100    0 -100    0    0 -100    0 -100    0 -100\n",
      "    0 -100    4 -100    0 -100 -100    0    0 -100    0 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100]\n"
     ]
    }
   ],
   "source": [
    "print('기존 원문 :', train_data_sentence[0])\n",
    "print('기존 레이블 :', train_data_label[0])\n",
    "print('-' * 50)\n",
    "print('토큰화 후 원문 :', [tokenizer.decode([word]) for word in X_train[0][0]])\n",
    "print('토큰화 후 레이블 :', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx\n",
    "in y_train[0]])\n",
    "print('-' * 50)\n",
    "print('정수 인코딩 결과 :', X_train[0][0])\n",
    "print('정수 인코딩 레이블 :', y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "id": "ipx8p0ZwFGC5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세그먼트 인코딩 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "어텐션 마스크 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('세그먼트 인코딩 :', X_train[2][0])\n",
    "print('어텐션 마스크 :', X_train[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "id": "C1VjgphVHWip"
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.1 # 드롭아웃 설정 (기본값: 0.1)\n",
    "\n",
    "class TFBertForTokenClassification(tf.keras.Model):\n",
    "  def __init__(self, model_name, num_labels):\n",
    "    super(TFBertForTokenClassification, self).__init__()\n",
    "    self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.classifier = tf.keras.layers.Dense(num_labels,\n",
    "                                            kernel_initializer=tf.keras.\n",
    "                                            initializers.TruncatedNormal\n",
    "                                            (0.02),\n",
    "                                            name='classifier')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    input_ids, attention_mask, token_type_ids = inputs\n",
    "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids)\n",
    "\n",
    "    # 전체 시퀀스에 대해서 분류해야하므로 outputs[0]임에 주의\n",
    "    all_output = outputs[0]\n",
    "    all_output = self.dropout(all_output)  # dropout 레이어 추가\n",
    "    prediction = self.classifier(all_output)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "id": "Srw3Sr2oI5_w"
   },
   "outputs": [],
   "source": [
    "labels = tf.constant([[-100, 2, 1, -100]])\n",
    "logits = tf.constant([[[0.8, 0.1, 0.1], [0.06, 0.04, 0.9], [0.75, 0.1, 0.15],\n",
    "  [0.4, 0.5, 0.1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "id": "FinSwSVJKKwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([False  True  True False], shape=(4,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "active_loss = tf.reshape(labels, (-1,)) != -100\n",
    "print(active_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "id": "QJ_Styg9KL6B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.06 0.04 0.9 ]\n",
      " [0.75 0.1  0.15]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])),\n",
    "  active_loss)\n",
    "print(reduced_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "id": "fMbN2NmrKlPQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "id": "qZH0Ykb7LcH5"
   },
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "  # 다중 클래스 분류 문제에서 소프트맥스 함수 미사용시 from_logits=True로 설정.\n",
    "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "  # -100의 값을 가진 정수에 대해서는 오차를 반영하지 않도록 labels를 수정.\n",
    "  active_loss = tf.reshape(labels, (-1,)) != -100\n",
    "\n",
    "  # activa_loss로부터 reduced_logits과 labels를 각각 얻는다.\n",
    "  reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2]))\n",
    "    , active_loss)\n",
    "  labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "  return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6utvCQAKMxwa"
   },
   "source": [
    "### 7단계: 모델 학습 및 평가 - KLUE-BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "id": "8u8xAak1QgHZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:48:53.499662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  model = TFBertForTokenClassification(\"klue/bert-base\", num_labels=tag_size) \n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  model.compile(optimizer=optimizer, loss=compute_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "id": "X_Ks3vk3M3dg"
   },
   "outputs": [],
   "source": [
    "class F1score(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, X_test, y_test):\n",
    "    self.X_test = X_test\n",
    "    self.y_test = y_test\n",
    "\n",
    "  def sequences_to_tags(self, label_ids, pred_ids):\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    for i in range(0, len(label_ids)):\n",
    "      label_tag = []\n",
    "      pred_tag = []\n",
    "\n",
    "      # 레이블의 값이 -100인 경우는 F1 score 계산시에도 제외\n",
    "      # ex) 레이블디코딩과정\n",
    "      # label_index : [1 -100 2 -100] ===> [1 2] ===> label_tag : [PER-B PER-I]\n",
    "      for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
    "        if label_index != -100:\n",
    "          label_tag.append(index_to_tag[label_index])\n",
    "          pred_tag.append(index_to_tag[pred_index])\n",
    "\n",
    "      label_list.append(label_tag)\n",
    "      pred_list.append(pred_tag)\n",
    "\n",
    "    return label_list, pred_list\n",
    "\n",
    "  # 에포크가 끝날때마다 실행되는 함수}\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    y_predicted = self.model.predict(self.X_test)\n",
    "    y_predicted = np.argmax(y_predicted, axis = 2)\n",
    "\n",
    "    label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
    "\n",
    "    score = f1_score(label_list, pred_list, suffix=True)\n",
    "    print(' - f1: {:04.2f}'.format(score * 100))\n",
    "    print(classification_report(label_list, pred_list, suffix=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8단계: 테스트데이터셋 평가(평가지표: F1-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "id": "SnvRtAGDO7Xh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:49:10.569713: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xd4331610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-13 16:49:10.569766: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2023-11-13 16:49:10.569779: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2023-11-13 16:49:10.586339: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-13 16:49:10.821752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-13 16:49:10.997797: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 8s 64ms/step\n",
      " - f1: 64.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FIL       0.91      0.89      0.90       831\n",
      "          PS       0.88      0.88      0.88        16\n",
      "         REP       0.76      0.78      0.77       848\n",
      "          WR       0.54      0.18      0.27      1294\n",
      "\n",
      "   micro avg       0.77      0.55      0.64      2989\n",
      "   macro avg       0.77      0.68      0.71      2989\n",
      "weighted avg       0.71      0.55      0.59      2989\n",
      "\n",
      "\n",
      "Epoch 1: saving model to training_2/cp.ckpt\n",
      "541/541 [==============================] - 148s 215ms/step - loss: 0.2021 - val_loss: 0.1639\n",
      "Epoch 2/10\n",
      "68/68 [==============================] - 5s 73ms/step\n",
      " - f1: 63.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FIL       0.92      0.88      0.90       831\n",
      "          PS       0.88      0.88      0.88        16\n",
      "         REP       0.73      0.81      0.77       848\n",
      "          WR       0.64      0.12      0.20      1294\n",
      "\n",
      "   micro avg       0.80      0.53      0.64      2989\n",
      "   macro avg       0.79      0.67      0.69      2989\n",
      "weighted avg       0.74      0.53      0.56      2989\n",
      "\n",
      "\n",
      "Epoch 2: saving model to training_2/cp.ckpt\n",
      "541/541 [==============================] - 92s 169ms/step - loss: 0.1367 - val_loss: 0.1818\n",
      "Epoch 3/10\n",
      "68/68 [==============================] - 5s 74ms/step\n",
      " - f1: 66.54\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FIL       0.91      0.90      0.90       831\n",
      "          PS       0.88      0.88      0.88        16\n",
      "         REP       0.80      0.77      0.78       848\n",
      "          WR       0.52      0.29      0.37      1294\n",
      "\n",
      "   micro avg       0.75      0.60      0.67      2989\n",
      "   macro avg       0.78      0.71      0.73      2989\n",
      "weighted avg       0.71      0.60      0.64      2989\n",
      "\n",
      "\n",
      "Epoch 3: saving model to training_2/cp.ckpt\n",
      "541/541 [==============================] - 90s 166ms/step - loss: 0.1003 - val_loss: 0.1778\n",
      "Epoch 4/10\n",
      "68/68 [==============================] - 5s 71ms/step\n",
      " - f1: 65.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FIL       0.91      0.88      0.90       831\n",
      "          PS       0.88      0.88      0.88        16\n",
      "         REP       0.73      0.80      0.77       848\n",
      "          WR       0.53      0.25      0.34      1294\n",
      "\n",
      "   micro avg       0.74      0.59      0.66      2989\n",
      "   macro avg       0.76      0.70      0.72      2989\n",
      "weighted avg       0.70      0.59      0.62      2989\n",
      "\n",
      "\n",
      "Epoch 4: saving model to training_2/cp.ckpt\n",
      "541/541 [==============================] - 90s 166ms/step - loss: 0.0703 - val_loss: 0.2060\n",
      "Epoch 5/10\n",
      "68/68 [==============================] - 5s 73ms/step\n",
      " - f1: 65.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FIL       0.90      0.88      0.89       831\n",
      "          PS       0.88      0.88      0.88        16\n",
      "         REP       0.79      0.76      0.77       848\n",
      "          WR       0.51      0.25      0.33      1294\n",
      "\n",
      "   micro avg       0.75      0.57      0.65      2989\n",
      "   macro avg       0.77      0.69      0.72      2989\n",
      "weighted avg       0.70      0.57      0.62      2989\n",
      "\n",
      "\n",
      "Epoch 5: saving model to training_2/cp.ckpt\n",
      "541/541 [==============================] - 89s 164ms/step - loss: 0.0489 - val_loss: 0.2502\n",
      "Epoch 6/10\n",
      "68/68 [==============================] - 5s 74ms/step\n",
      " - f1: 65.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FIL       0.90      0.91      0.90       831\n",
      "          PS       0.88      0.88      0.88        16\n",
      "         REP       0.82      0.73      0.77       848\n",
      "          WR       0.53      0.23      0.32      1294\n",
      "\n",
      "   micro avg       0.77      0.57      0.65      2989\n",
      "   macro avg       0.78      0.69      0.72      2989\n",
      "weighted avg       0.71      0.57      0.61      2989\n",
      "\n",
      "\n",
      "Epoch 6: saving model to training_2/cp.ckpt\n",
      "541/541 [==============================] - 88s 162ms/step - loss: 0.0349 - val_loss: 0.2834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f54a07aba60>"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "\n",
    "# 모델학습 과정에서의 테스트데이터셋 대상 모델 성능 모니터링\n",
    "f1_score_report = F1score(X_test, y_test)\n",
    "\n",
    "# 조기 종료\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 모델 체크포인트\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# 모델 가중치를 저장하는 콜백 \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "model.fit(\n",
    "  X_train, y_train, epochs=10, batch_size=32,\n",
    "  validation_data=(X_valid, y_valid),\n",
    "  callbacks = [f1_score_report, early_stopping, cp_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp.ckpt.index', 'checkpoint', 'cp.ckpt.data-00000-of-00001']"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 체크포인트 파일 확인\n",
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_bert_model (TFBertModel  multiple                  110617344 \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110621189 (421.99 MB)\n",
      "Trainable params: 110621189 (421.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 아키텍처 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 5), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc054730>, 140006822039824), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 5), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc054730>, 140006822039824), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(5,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc06a940>, 140006822042736), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(5,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc06a940>, 140006822042736), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 5), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc054730>, 140006822039824), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(None, 5), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc054730>, 140006822039824), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(5,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc06a940>, 140006822042736), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(5,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f54cc06a940>, 140006822042736), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/kluebert_base_new/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/kluebert_base_new/assets\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model.save('saved_model/kluebert_base_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "id": "FueVUx16PaNx"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
    "                                                  pad_token_id_for_segment=0,\n",
    "                                                  pad_token_id_for_label=-100):\n",
    "  cls_token = tokenizer.cls_token\n",
    "  sep_token = tokenizer.sep_token\n",
    "  pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "  input_ids, attention_masks, token_type_ids, label_masks = [], [], [], []\n",
    "\n",
    "  for example in tqdm(examples):\n",
    "    tokens = []\n",
    "    label_mask = []\n",
    "    for one_word in example:\n",
    "      # 하나의 단어에 대해서 서브워드로 토큰화\n",
    "      subword_tokens = tokenizer.tokenize(one_word)\n",
    "      tokens.extend(subword_tokens)\n",
    "      # 서브워드 중 첫번째 서브워드만 개체명 레이블을 부여하고 그 외에는 -100으로 채운다.\n",
    "      label_mask.extend([0]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
    "\n",
    "    # [CLS]와 [SEP]를 후에 추가할 것을 고려하여 최대길이를 초과하는 샘플의 경우 max_seq_len - 2의 길이로 변환.\n",
    "    # ex) max_seq_len = 64라면 길이가 62보다 긴 샘플은 뒷 부분을 자르고 길이 62로 변 환.\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > max_seq_len - special_tokens_count:\n",
    "      tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "      label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # [SEP]를 추가하는 코드\n",
    "    # 1. 토큰화 결과의 맨 뒷부분에 [SEP] 토큰 추가\n",
    "    # 2. 레이블에도 맨뒷부분에 -100 추가.\n",
    "    tokens += [sep_token]\n",
    "    label_mask += [pad_token_id_for_label]\n",
    "\n",
    "    # [CLS]를 추가하는 코드\n",
    "    # 1. 토큰화 결과의 앞부분에 [CLS] 토큰 추가\n",
    "    # 2. 레이블의 맨앞부분에도 -100 추가.\n",
    "    tokens = [cls_token] + tokens\n",
    "    label_mask = [pad_token_id_for_label] + label_mask\n",
    "\n",
    "    # 정수인코딩\n",
    "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # 어텐션 마스크 생성\n",
    "    attention_mask = [1] * len(input_id)\n",
    "\n",
    "    # 정수 인코딩에 추가할 패딩 길이 연산\n",
    "    padding_count = max_seq_len - len(input_id)\n",
    "\n",
    "    # 정수인코딩 ,어텐션 마스크에 패딩 추가\n",
    "    input_id = input_id + ([pad_token_id] * padding_count)\n",
    "    attention_mask = attention_mask + ([0] * padding_count)\n",
    "\n",
    "    # 세그먼트 인코딩.\n",
    "    token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "\n",
    "    # 레이블 패딩. (단, 이 경우는 패딩 토큰의 ID가 -100)\n",
    "    label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n",
    "\n",
    "    assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "    assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "    assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "    assert len(label_mask) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label_mask), max_seq_len)\n",
    "\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "    label_masks.append(label_mask)\n",
    "\n",
    "  input_ids = np.array(input_ids, dtype=int)\n",
    "  attention_masks = np.array(attention_masks, dtype=int)\n",
    "  token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "  label_masks = np.asarray(label_masks, dtype=np.int32)\n",
    "\n",
    "  return (input_ids, attention_masks, token_type_ids), label_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "id": "3b3S_ezbSoua"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2191.38it/s]\n"
     ]
    }
   ],
   "source": [
    "X_pred, label_masks = convert_examples_to_features_for_prediction(test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "id": "e1sYh79uSs_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 원문 : ['부부로서의', '가족', '책임과', '의무를', '피하고', '개인적인', '목표와', '꿈에', '집중할', '수', '있기', '때문입니다']\n",
      "--------------------------------------------------\n",
      "토큰화 후 원문 : ['[CLS]', '부부', '##로', '##서', '##의', '가족', '책임', '##과', '의무', '##를', '피하', '##고', '개인', '##적인', '목표', '##와', '꿈', '##에', '집중', '##할', '수', '있', '##기', '때문', '##입니다', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "레이블 마스크 : ['[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[FIRST]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[FIRST]', '[FIRST]', '[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print('기존 원문 :', test_data_sentence[0])\n",
    "print('-' * 50)\n",
    "print('토큰화 후 원문 :', [tokenizer.decode([word]) for word in X_pred[0][0]])\n",
    "print('레이블 마스크 :', ['[PAD]' if idx == -100 else '[FIRST]' for idx in\n",
    "label_masks[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "id": "DVBcRESvS2UI"
   },
   "outputs": [],
   "source": [
    "def ner_prediction(examples, max_seq_len, tokenizer):\n",
    "  examples = [sent.split() for sent in examples]\n",
    "  X_pred, label_masks = convert_examples_to_features_for_prediction(examples,\n",
    "  max_seq_len=128, tokenizer=tokenizer)\n",
    "  y_predicted = model.predict(X_pred)\n",
    "  y_predicted = np.argmax(y_predicted, axis = 2)\n",
    "\n",
    "  pred_list = []\n",
    "  result_list = []\n",
    "\n",
    "  for i in range(0, len(label_masks)):\n",
    "    pred_tag = []\n",
    "    # ex) 모델의 예측값 디코딩 과정\n",
    "    # 예측값(y_predicted)에서 레이블마스크(label_masks)의 값이 -100인 동일 위치의 값을 삭제\n",
    "    # label_masks : [-100 0 -100 0 -100]\n",
    "    # y_predicted : [ 0 1 0 2 0 ] ==> [1 2] ==> 최 종 예 측(pred_tag) : [PER\u0002B PER-I]\n",
    "    for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
    "      if label_index != -100:\n",
    "        pred_tag.append(index_to_tag[pred_index])\n",
    "\n",
    "    pred_list.append(pred_tag)\n",
    "\n",
    "  for example, pred in zip(examples, pred_list):\n",
    "    one_sample_result = []\n",
    "    for one_word, label_token in zip(example, pred):\n",
    "      one_sample_result.append((one_word, label_token))\n",
    "    result_list.append(one_sample_result)\n",
    "\n",
    "  return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9단계: 테스트데이터셋 중 샘플 문장 2개 평가 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p6YpBtUS653"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1146.14it/s]\n"
     ]
    }
   ],
   "source": [
    "sent1 = '음   셋째로,  익명성은  사회성사생활  침해와 개인정보 유출을  어   가속가속화시킬  수 있습니다'\n",
    "sent2 = '만약  기침이나 재채기를  <PS/>  기침이나 재채기를  해야 할 경우에는  어  손으로 입과  코  가리고 휴지나 팔꿈치 등으로 치켜서 하여 바이러스의 분사를 최소화해야 합니다'\n",
    "test_samples = [sent1, sent2]\n",
    "result_list = ner_prediction(test_samples, max_seq_len=128, tokenizer=tokenizer)\n",
    "result_list\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
